{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"font-size:70px; color:red; font-weight:700;\">Using GPT 2</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding: 100%|██████████| 24/24 [00:41<00:00,  1.71s/it]\n",
      "Embedding: 100%|██████████| 1/1 [00:00<00:00, 58.98it/s]\n",
      "Embedding: 100%|██████████| 1/1 [00:00<00:00, 56.05it/s]\n",
      "Embedding: 100%|██████████| 1/1 [00:00<00:00, 57.07it/s]\n",
      "Embedding: 100%|██████████| 1/1 [00:00<00:00, 52.62it/s]\n",
      "Embedding: 100%|██████████| 1/1 [00:00<00:00, 55.47it/s]\n",
      "Embedding: 100%|██████████| 1/1 [00:00<00:00, 62.47it/s]\n",
      "Embedding: 100%|██████████| 1/1 [00:00<00:00, 66.08it/s]\n",
      "Embedding: 100%|██████████| 1/1 [00:00<00:00, 54.00it/s]\n",
      "Embedding: 100%|██████████| 1/1 [00:00<00:00, 52.22it/s]\n",
      "Embedding: 100%|██████████| 1/1 [00:00<00:00, 49.92it/s]\n",
      "Embedding: 100%|██████████| 1/1 [00:00<00:00, 48.69it/s]\n",
      "Embedding: 100%|██████████| 1/1 [00:00<00:00, 48.74it/s]\n",
      "Embedding: 100%|██████████| 1/1 [00:00<00:00, 53.97it/s]\n",
      "Embedding: 100%|██████████| 1/1 [00:00<00:00, 66.54it/s]\n",
      "Embedding: 100%|██████████| 1/1 [00:00<00:00, 66.51it/s]\n",
      "Embedding: 100%|██████████| 1/1 [00:00<00:00, 64.41it/s]\n",
      "Embedding: 100%|██████████| 1/1 [00:00<00:00, 52.61it/s]\n",
      "Embedding: 100%|██████████| 1/1 [00:00<00:00, 57.06it/s]\n",
      "Embedding: 100%|██████████| 1/1 [00:00<00:00, 44.17it/s]\n",
      "Embedding: 100%|██████████| 1/1 [00:00<00:00, 39.93it/s]\n",
      "Embedding: 100%|██████████| 1/1 [00:00<00:00, 40.78it/s]\n",
      "Embedding: 100%|██████████| 1/1 [00:00<00:00, 48.74it/s]\n",
      "Embedding: 100%|██████████| 1/1 [00:00<00:00, 58.81it/s]\n",
      "Embedding: 100%|██████████| 1/1 [00:00<00:00, 71.39it/s]\n",
      "Embedding: 100%|██████████| 1/1 [00:00<00:00, 62.48it/s]\n",
      "Embedding: 100%|██████████| 1/1 [00:00<00:00, 39.93it/s]\n",
      "Embedding: 100%|██████████| 1/1 [00:00<00:00, 43.47it/s]\n",
      "Embedding: 100%|██████████| 1/1 [00:00<00:00, 63.76it/s]\n",
      "Embedding: 100%|██████████| 1/1 [00:00<00:00, 62.49it/s]\n",
      "Embedding: 100%|██████████| 1/1 [00:00<00:00, 51.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 BM25 Eval: {'MAP@10': 0.3424338624338624, 'MRR@10': 0.3424338624338624, 'nDCG@10': 0.48791479025996576}\n",
      "🔍 Dense Eval: {'MAP@10': 0.2853042328042328, 'MRR@10': 0.2853042328042328, 'nDCG@10': 0.44621179937295}\n",
      "* Running on local URL:  http://127.0.0.1:7861\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding: 100%|██████████| 1/1 [00:00<00:00, 28.22it/s]\n",
      "c:\\Python312\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:567: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "Both `max_new_tokens` (=150) and `max_length`(=900) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Embedding: 100%|██████████| 1/1 [00:00<00:00, 33.99it/s]\n",
      "Both `max_new_tokens` (=150) and `max_length`(=900) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Embedding: 100%|██████████| 1/1 [00:00<00:00, 38.89it/s]\n",
      "Both `max_new_tokens` (=150) and `max_length`(=900) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Embedding: 100%|██████████| 1/1 [00:00<00:00, 25.44it/s]\n",
      "Both `max_new_tokens` (=150) and `max_length`(=900) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Embedding: 100%|██████████| 1/1 [00:00<00:00, 33.76it/s]\n",
      "Llama.generate: 9 prefix-match hit, remaining 319 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2965.17 ms\n",
      "llama_perf_context_print: prompt eval time =    3047.94 ms /   319 tokens (    9.55 ms per token,   104.66 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1645.82 ms /    68 runs   (   24.20 ms per token,    41.32 tokens per second)\n",
      "llama_perf_context_print:       total time =    4723.43 ms /   387 tokens\n",
      "Embedding: 100%|██████████| 1/1 [00:00<00:00, 29.34it/s]\n",
      "Llama.generate: 9 prefix-match hit, remaining 415 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2965.17 ms\n",
      "llama_perf_context_print: prompt eval time =    3658.76 ms /   415 tokens (    8.82 ms per token,   113.43 tokens per second)\n",
      "llama_perf_context_print:        eval time =     570.31 ms /    23 runs   (   24.80 ms per token,    40.33 tokens per second)\n",
      "llama_perf_context_print:       total time =    4237.85 ms /   438 tokens\n",
      "Embedding: 100%|██████████| 1/1 [00:00<00:00, 40.33it/s]\n",
      "Llama.generate: 9 prefix-match hit, remaining 376 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2965.17 ms\n",
      "llama_perf_context_print: prompt eval time =    3165.00 ms /   376 tokens (    8.42 ms per token,   118.80 tokens per second)\n",
      "llama_perf_context_print:        eval time =    3163.74 ms /   126 runs   (   25.11 ms per token,    39.83 tokens per second)\n",
      "llama_perf_context_print:       total time =    6392.23 ms /   502 tokens\n",
      "Embedding: 100%|██████████| 1/1 [00:00<00:00, 29.56it/s]\n",
      "Llama.generate: 9 prefix-match hit, remaining 381 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2965.17 ms\n",
      "llama_perf_context_print: prompt eval time =    3015.98 ms /   381 tokens (    7.92 ms per token,   126.33 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2879.76 ms /   121 runs   (   23.80 ms per token,    42.02 tokens per second)\n",
      "llama_perf_context_print:       total time =    5954.77 ms /   502 tokens\n",
      "Embedding: 100%|██████████| 1/1 [00:00<00:00, 28.72it/s]\n",
      "Llama.generate: 9 prefix-match hit, remaining 399 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2965.17 ms\n",
      "llama_perf_context_print: prompt eval time =    3282.57 ms /   399 tokens (    8.23 ms per token,   121.55 tokens per second)\n",
      "llama_perf_context_print:        eval time =     778.83 ms /    32 runs   (   24.34 ms per token,    41.09 tokens per second)\n",
      "llama_perf_context_print:       total time =    4073.96 ms /   431 tokens\n",
      "Embedding: 100%|██████████| 1/1 [00:00<00:00, 15.79it/s]\n",
      "Llama.generate: 9 prefix-match hit, remaining 288 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2965.17 ms\n",
      "llama_perf_context_print: prompt eval time =    2291.16 ms /   288 tokens (    7.96 ms per token,   125.70 tokens per second)\n",
      "llama_perf_context_print:        eval time =    4275.48 ms /   180 runs   (   23.75 ms per token,    42.10 tokens per second)\n",
      "llama_perf_context_print:       total time =    6672.29 ms /   468 tokens\n",
      "Embedding: 100%|██████████| 1/1 [00:00<00:00, 33.18it/s]\n",
      "Both `max_new_tokens` (=150) and `max_length`(=900) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Embedding: 100%|██████████| 1/1 [00:00<00:00, 34.56it/s]\n",
      "Both `max_new_tokens` (=150) and `max_length`(=900) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Embedding: 100%|██████████| 1/1 [00:00<00:00, 34.49it/s]\n",
      "Both `max_new_tokens` (=150) and `max_length`(=900) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Embedding: 100%|██████████| 1/1 [00:00<00:00, 26.20it/s]\n",
      "Both `max_new_tokens` (=150) and `max_length`(=900) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Embedding: 100%|██████████| 1/1 [00:00<00:00, 41.55it/s]\n",
      "Llama.generate: 9 prefix-match hit, remaining 399 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2965.17 ms\n",
      "llama_perf_context_print: prompt eval time =    3313.49 ms /   399 tokens (    8.30 ms per token,   120.42 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1919.16 ms /    77 runs   (   24.92 ms per token,    40.12 tokens per second)\n",
      "llama_perf_context_print:       total time =    5268.39 ms /   476 tokens\n",
      "Embedding: 100%|██████████| 1/1 [00:00<00:00, 38.19it/s]\n",
      "Llama.generate: 9 prefix-match hit, remaining 288 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2965.17 ms\n",
      "llama_perf_context_print: prompt eval time =    2344.26 ms /   288 tokens (    8.14 ms per token,   122.85 tokens per second)\n",
      "llama_perf_context_print:        eval time =    4939.33 ms /   214 runs   (   23.08 ms per token,    43.33 tokens per second)\n",
      "llama_perf_context_print:       total time =    7418.12 ms /   502 tokens\n",
      "Embedding: 100%|██████████| 1/1 [00:00<00:00, 36.69it/s]\n",
      "Llama.generate: 10 prefix-match hit, remaining 414 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2965.17 ms\n",
      "llama_perf_context_print: prompt eval time =    3473.68 ms /   414 tokens (    8.39 ms per token,   119.18 tokens per second)\n",
      "llama_perf_context_print:        eval time =     546.20 ms /    22 runs   (   24.83 ms per token,    40.28 tokens per second)\n",
      "llama_perf_context_print:       total time =    4028.01 ms /   436 tokens\n",
      "Embedding: 100%|██████████| 1/1 [00:00<00:00, 37.42it/s]\n",
      "Llama.generate: 9 prefix-match hit, remaining 498 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2965.17 ms\n",
      "llama_perf_context_print: prompt eval time =    4082.84 ms /   498 tokens (    8.20 ms per token,   121.97 tokens per second)\n",
      "llama_perf_context_print:        eval time =     110.65 ms /     4 runs   (   27.66 ms per token,    36.15 tokens per second)\n",
      "llama_perf_context_print:       total time =    4195.69 ms /   502 tokens\n",
      "Embedding: 100%|██████████| 1/1 [00:00<00:00, 31.25it/s]\n",
      "Llama.generate: 9 prefix-match hit, remaining 381 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2965.17 ms\n",
      "llama_perf_context_print: prompt eval time =    3216.95 ms /   381 tokens (    8.44 ms per token,   118.44 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2863.74 ms /   121 runs   (   23.67 ms per token,    42.25 tokens per second)\n",
      "llama_perf_context_print:       total time =    6139.29 ms /   502 tokens\n",
      "Embedding: 100%|██████████| 1/1 [00:00<00:00, 27.38it/s]\n",
      "Both `max_new_tokens` (=150) and `max_length`(=900) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Embedding: 100%|██████████| 1/1 [00:00<00:00, 30.63it/s]\n",
      "Llama.generate: 9 prefix-match hit, remaining 399 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2965.17 ms\n",
      "llama_perf_context_print: prompt eval time =    3093.55 ms /   399 tokens (    7.75 ms per token,   128.98 tokens per second)\n",
      "llama_perf_context_print:        eval time =      34.93 ms /     1 runs   (   34.93 ms per token,    28.63 tokens per second)\n",
      "llama_perf_context_print:       total time =    3130.04 ms /   400 tokens\n",
      "Embedding: 100%|██████████| 1/1 [00:00<00:00, 29.34it/s]\n",
      "Llama.generate: 407 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2965.17 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    2373.02 ms /   104 runs   (   22.82 ms per token,    43.83 tokens per second)\n",
      "llama_perf_context_print:       total time =    2417.49 ms /   105 tokens\n",
      "Embedding: 100%|██████████| 1/1 [00:00<00:00,  9.48it/s]\n",
      "Llama.generate: 9 prefix-match hit, remaining 319 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2965.17 ms\n",
      "llama_perf_context_print: prompt eval time =    4315.89 ms /   319 tokens (   13.53 ms per token,    73.91 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1248.80 ms /    57 runs   (   21.91 ms per token,    45.64 tokens per second)\n",
      "llama_perf_context_print:       total time =    5587.39 ms /   376 tokens\n",
      "Embedding: 100%|██████████| 1/1 [00:00<00:00, 36.03it/s]\n",
      "Both `max_new_tokens` (=150) and `max_length`(=900) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Embedding: 100%|██████████| 1/1 [00:00<00:00, 33.14it/s]\n",
      "Both `max_new_tokens` (=150) and `max_length`(=900) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Embedding: 100%|██████████| 1/1 [00:00<00:00, 32.77it/s]\n",
      "Llama.generate: 9 prefix-match hit, remaining 288 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2965.17 ms\n",
      "llama_perf_context_print: prompt eval time =    2293.32 ms /   288 tokens (    7.96 ms per token,   125.58 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2597.74 ms /   114 runs   (   22.79 ms per token,    43.88 tokens per second)\n",
      "llama_perf_context_print:       total time =    4943.91 ms /   402 tokens\n",
      "Embedding: 100%|██████████| 1/1 [00:00<00:00, 31.71it/s]\n",
      "Llama.generate: 10 prefix-match hit, remaining 414 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2965.17 ms\n",
      "llama_perf_context_print: prompt eval time =    2957.61 ms /   414 tokens (    7.14 ms per token,   139.98 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1612.49 ms /    72 runs   (   22.40 ms per token,    44.65 tokens per second)\n",
      "llama_perf_context_print:       total time =    4597.97 ms /   486 tokens\n",
      "Embedding: 100%|██████████| 1/1 [00:00<00:00, 31.27it/s]\n",
      "Both `max_new_tokens` (=150) and `max_length`(=900) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Embedding: 100%|██████████| 1/1 [00:00<00:00, 35.07it/s]\n",
      "Both `max_new_tokens` (=150) and `max_length`(=900) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Embedding: 100%|██████████| 1/1 [00:00<00:00, 33.86it/s]\n",
      "Llama.generate: 9 prefix-match hit, remaining 381 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2965.17 ms\n",
      "llama_perf_context_print: prompt eval time =    2795.33 ms /   381 tokens (    7.34 ms per token,   136.30 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1577.91 ms /    65 runs   (   24.28 ms per token,    41.19 tokens per second)\n",
      "llama_perf_context_print:       total time =    4400.11 ms /   446 tokens\n"
     ]
    }
   ],
   "source": [
    "# hybrid_qa_app.py\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import torch\n",
    "import faiss\n",
    "import nltk\n",
    "import gradio as gr\n",
    "import pytrec_eval\n",
    "from bs4 import BeautifulSoup\n",
    "from rank_bm25 import BM25Okapi\n",
    "from transformers import AutoTokenizer, AutoModel, GPT2Tokenizer, GPT2LMHeadModel\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "# 1) Force CPU\n",
    "DEVICE = torch.device(\"cpu\")\n",
    "\n",
    "# 2) Ensure NLTK tokenizer is available\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "except LookupError:\n",
    "    nltk.download(\"punkt\")\n",
    "\n",
    "# 3) Paths\n",
    "DATA_DIR = \"stacklite_dataset\"\n",
    "DATA_FILE = os.path.join(DATA_DIR, \"top_datascience_questions.json\")\n",
    "QUERY_FILE = os.path.join(DATA_DIR, \"queries.json\")\n",
    "QRELS_FILE = os.path.join(DATA_DIR, \"qrels.json\")\n",
    "\n",
    "# 4) Load and clean data\n",
    "with open(DATA_FILE, 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "def clean_text(html):\n",
    "    return BeautifulSoup(html, \"html.parser\").get_text(separator=\" \")\n",
    "\n",
    "df['text'] = df.apply(lambda r: r['title'] + \" \" + clean_text(r['body']), axis=1)\n",
    "\n",
    "# 5) BM25 setup\n",
    "def safe_tokenize(text):\n",
    "    try:\n",
    "        return nltk.word_tokenize(text.lower())\n",
    "    except LookupError:\n",
    "        return re.findall(r'\\b\\w+\\b', text.lower())\n",
    "\n",
    "tokenized_corpus = [safe_tokenize(doc) for doc in df['text']]\n",
    "bm25 = BM25Okapi(tokenized_corpus)\n",
    "\n",
    "# 6) Dense Retrieval (MiniLM + FAISS)\n",
    "EMBED_MODEL_NAME = 'sentence-transformers/all-MiniLM-L6-v2'\n",
    "embed_tokenizer = AutoTokenizer.from_pretrained(EMBED_MODEL_NAME)\n",
    "embed_model = AutoModel.from_pretrained(EMBED_MODEL_NAME).to(DEVICE)\n",
    "\n",
    "def get_embeddings(texts, batch_size=32):\n",
    "    all_embs = []\n",
    "    for i in tqdm(range(0, len(texts), batch_size), desc=\"Embedding\"):\n",
    "        batch = texts[i:i+batch_size]\n",
    "        inputs = embed_tokenizer(batch, padding=True, truncation=True, return_tensors=\"pt\", max_length=512).to(DEVICE)\n",
    "        with torch.no_grad():\n",
    "            out = embed_model(**inputs)\n",
    "        last_hidden = out.last_hidden_state\n",
    "        mask = inputs.attention_mask.unsqueeze(-1).expand(last_hidden.size()).float()\n",
    "        summed = torch.sum(last_hidden * mask, 1)\n",
    "        counts = torch.clamp(mask.sum(1), min=1e-9)\n",
    "        avg = (summed / counts).cpu().numpy()\n",
    "        all_embs.append(avg)\n",
    "    return np.vstack(all_embs)\n",
    "\n",
    "dense_embeddings = get_embeddings(df['text'].tolist())\n",
    "dim = dense_embeddings.shape[1]\n",
    "index = faiss.IndexFlatL2(dim)\n",
    "index.add(dense_embeddings.astype(np.float32))\n",
    "\n",
    "# 7) RRF Fusion\n",
    "def rrf(bm25_ids, dense_ids, k=60):\n",
    "    from collections import defaultdict\n",
    "    scores = defaultdict(float)\n",
    "    for rank, idx in enumerate(bm25_ids):\n",
    "        scores[idx] += 1 / (k + rank + 1)\n",
    "    for rank, idx in enumerate(dense_ids):\n",
    "        scores[idx] += 1 / (k + rank + 1)\n",
    "    return sorted(scores, key=scores.get, reverse=True)\n",
    "\n",
    "# 8) Generator (GPT-2 with its own tokenizer)\n",
    "GEN_MODEL = \"gpt2\"\n",
    "gen_tokenizer = GPT2Tokenizer.from_pretrained(GEN_MODEL)\n",
    "gen_tokenizer.pad_token = gen_tokenizer.eos_token\n",
    "gen_model = GPT2LMHeadModel.from_pretrained(GEN_MODEL).to(DEVICE)\n",
    "\n",
    "# 9) Retrieval functions\n",
    "def retrieve_bM25(question, k=10):\n",
    "    tok = safe_tokenize(question)\n",
    "    sims = bm25.get_scores(tok)\n",
    "    idxs = np.argsort(sims)[::-1][:k]\n",
    "    return [str(df.iloc[i]['question_id']) for i in idxs if i < len(df)]\n",
    "\n",
    "def retrieve_dense(question, k=10):\n",
    "    emb = get_embeddings([question])\n",
    "    _, idxs = index.search(emb.astype(np.float32), k)\n",
    "    return [str(df.iloc[i]['question_id']) for i in idxs[0] if i < len(df)]\n",
    "\n",
    "# 10) Answer generation with safe indexing and length constraints\n",
    "def hybrid_answer(query):\n",
    "    k_bm25 = 30\n",
    "    k_dense = 30\n",
    "    top_k = 10\n",
    "\n",
    "    bm_scores = bm25.get_scores(safe_tokenize(query))\n",
    "    bm_idx = np.argsort(bm_scores)[::-1][:k_bm25]\n",
    "    bm_idx = [idx for idx in bm_idx if idx < len(df)]\n",
    "\n",
    "    dense_emb = get_embeddings([query])\n",
    "    _, dense_idx = index.search(dense_emb.astype(np.float32), k_dense)\n",
    "    dense_idx = [idx for idx in dense_idx[0] if idx < len(df)]\n",
    "\n",
    "    fused = rrf(bm_idx, dense_idx)[:top_k]\n",
    "\n",
    "    docs = []\n",
    "    for rank, idx in enumerate(fused):\n",
    "        try:\n",
    "            words = df.iloc[idx]['text'].split()[:100]\n",
    "            docs.append(f\"[Doc {rank+1}]: \" + \" \".join(words))\n",
    "        except IndexError:\n",
    "            continue  # Skip if idx is out of bounds\n",
    "\n",
    "    context = \"\\n\\n\".join(docs)\n",
    "    prompt = f\"Context:\\n{context}\\n\\nQuestion: {query}\\nAnswer:\"\n",
    "\n",
    "    # Tokenize with length constraints\n",
    "    inputs = gen_tokenizer(\n",
    "        prompt, \n",
    "        return_tensors=\"pt\", \n",
    "        truncation=True, \n",
    "        max_length=768  # Reserve space for generation\n",
    "    )\n",
    "    input_ids = inputs.input_ids.to(DEVICE)\n",
    "    attention_mask = inputs.attention_mask.to(DEVICE)\n",
    "\n",
    "    # Generate answer with safe length limits\n",
    "    try:\n",
    "        output_sequences = gen_model.generate(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            max_new_tokens=150,\n",
    "            max_length=900,  # Total length < 1024\n",
    "            pad_token_id=gen_tokenizer.eos_token_id,\n",
    "            temperature=0.7,\n",
    "            num_return_sequences=1\n",
    "        )\n",
    "        generated_sequence = output_sequences[0, input_ids.shape[1]:].tolist()\n",
    "        answer = gen_tokenizer.decode(generated_sequence, skip_special_tokens=True).strip()\n",
    "    except Exception as e:\n",
    "        answer = f\"Error in generation: {str(e)}\"\n",
    "\n",
    "    cits = []\n",
    "    for i, idx in enumerate(fused[:5]):\n",
    "        try:\n",
    "            row = df.iloc[idx]\n",
    "            cits.append(f\"{i+1}. {row['title']} (ID: {row['question_id']})\")\n",
    "        except IndexError:\n",
    "            continue\n",
    "\n",
    "    return answer, \"\\n\".join(cits)\n",
    "\n",
    "# 11) Gradio interface\n",
    "example_questions = [\n",
    "    \"How to handle missing values in a dataset?\",\n",
    "    \"What's the difference between random forest and gradient boosting?\",\n",
    "    \"How to implement a neural network in PyTorch?\",\n",
    "    \"When should I use L1 vs L2 regularization?\",\n",
    "    \"How does PCA reduce dimensionality?\",\n",
    "    \"Explain cross-validation and why it's important.\",\n",
    "    \"How to optimize hyperparameters in scikit-learn?\",\n",
    "    \"What is the bias-variance tradeoff?\",\n",
    "    \"How to deploy a model with Flask?\",\n",
    "    \"What are attention mechanisms in transformers?\"\n",
    "]\n",
    "\n",
    "iface = gr.Interface(\n",
    "    fn=hybrid_answer,\n",
    "    inputs=gr.Textbox(label=\"Technical Question\", placeholder=\"Ask anything…\"),\n",
    "    outputs=[gr.Textbox(label=\"Answer\"), gr.Textbox(label=\"Citations\")],\n",
    "    title=\"📚 Hybrid QA Assistant (StackLite)\",\n",
    "    description=\"BM25 + MiniLM + RRF fusion + GPT-2 on CPU\",\n",
    "    examples=example_questions\n",
    ")\n",
    "\n",
    "# 12) Main\n",
    "if __name__ == \"__main__\":\n",
    "    with open(QUERY_FILE) as f:\n",
    "        queries = json.load(f)\n",
    "    with open(QRELS_FILE) as f:\n",
    "        raw_qrels = json.load(f)\n",
    "    qrels = {str(qid): {str(doc): 1 for doc in docs} for qid, docs in raw_qrels.items()}\n",
    "\n",
    "    def build_run(fn):\n",
    "        return {str(q['id']): {doc: 1.0 for doc in fn(q['question'])} for q in queries}\n",
    "\n",
    "    run_bm25 = build_run(retrieve_bM25)\n",
    "    run_dense = build_run(retrieve_dense)\n",
    "\n",
    "    evaluator = pytrec_eval.RelevanceEvaluator(qrels, {'map', 'recip_rank', 'ndcg'})\n",
    "    metrics_bm25 = evaluator.evaluate(run_bm25)\n",
    "    metrics_dense = evaluator.evaluate(run_dense)\n",
    "\n",
    "    def summarize(metrics):\n",
    "        return {\n",
    "            'MAP@10': np.mean([v['map'] for v in metrics.values()]),\n",
    "            'MRR@10': np.mean([v['recip_rank'] for v in metrics.values()]),\n",
    "            'nDCG@10': np.mean([v['ndcg'] for v in metrics.values()])\n",
    "        }\n",
    "\n",
    "    print(\"🔍 BM25 Eval:\", summarize(metrics_bm25))\n",
    "    print(\"🔍 Dense Eval:\", summarize(metrics_dense))\n",
    "\n",
    "    iface.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for me i used tiny llama which scored better than gpt 2 ang got better answers i tried both"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"font-size:70px; color:red; font-weight:700;\">Using tinyllama</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\marwan\\AppData\\Roaming\\Python\\Python312\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e6e3a4064de4975b20348f310321827",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 23 key-value pairs and 201 tensors from ./models/tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = tinyllama_tinyllama-1.1b-chat-v1.0\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 2048\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 2048\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 22\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 5632\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 64\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 4\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 10000.000000\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  16:                      tokenizer.ggml.merges arr[str,61249]   = [\"▁ t\", \"e r\", \"i n\", \"▁ a\", \"e n...\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  18:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  20:            tokenizer.ggml.padding_token_id u32              = 2\n",
      "llama_model_loader: - kv  21:                    tokenizer.chat_template str              = {% for message in messages %}\\n{% if m...\n",
      "llama_model_loader: - kv  22:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   45 tensors\n",
      "llama_model_loader: - type q4_K:  135 tensors\n",
      "llama_model_loader: - type q6_K:   21 tensors\n",
      "print_info: file format = GGUF V3 (latest)\n",
      "print_info: file type   = Q4_K - Medium\n",
      "print_info: file size   = 636.18 MiB (4.85 BPW) \n",
      "init_tokenizer: initializing tokenizer for type 1\n",
      "load: control token:      2 '</s>' is not marked as EOG\n",
      "load: control token:      1 '<s>' is not marked as EOG\n",
      "load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
      "load: special tokens cache size = 3\n",
      "load: token to piece cache size = 0.1684 MB\n",
      "print_info: arch             = llama\n",
      "print_info: vocab_only       = 0\n",
      "print_info: n_ctx_train      = 2048\n",
      "print_info: n_embd           = 2048\n",
      "print_info: n_layer          = 22\n",
      "print_info: n_head           = 32\n",
      "print_info: n_head_kv        = 4\n",
      "print_info: n_rot            = 64\n",
      "print_info: n_swa            = 0\n",
      "print_info: n_swa_pattern    = 1\n",
      "print_info: n_embd_head_k    = 64\n",
      "print_info: n_embd_head_v    = 64\n",
      "print_info: n_gqa            = 8\n",
      "print_info: n_embd_k_gqa     = 256\n",
      "print_info: n_embd_v_gqa     = 256\n",
      "print_info: f_norm_eps       = 0.0e+00\n",
      "print_info: f_norm_rms_eps   = 1.0e-05\n",
      "print_info: f_clamp_kqv      = 0.0e+00\n",
      "print_info: f_max_alibi_bias = 0.0e+00\n",
      "print_info: f_logit_scale    = 0.0e+00\n",
      "print_info: f_attn_scale     = 0.0e+00\n",
      "print_info: n_ff             = 5632\n",
      "print_info: n_expert         = 0\n",
      "print_info: n_expert_used    = 0\n",
      "print_info: causal attn      = 1\n",
      "print_info: pooling type     = 0\n",
      "print_info: rope type        = 0\n",
      "print_info: rope scaling     = linear\n",
      "print_info: freq_base_train  = 10000.0\n",
      "print_info: freq_scale_train = 1\n",
      "print_info: n_ctx_orig_yarn  = 2048\n",
      "print_info: rope_finetuned   = unknown\n",
      "print_info: ssm_d_conv       = 0\n",
      "print_info: ssm_d_inner      = 0\n",
      "print_info: ssm_d_state      = 0\n",
      "print_info: ssm_dt_rank      = 0\n",
      "print_info: ssm_dt_b_c_rms   = 0\n",
      "print_info: model type       = 1B\n",
      "print_info: model params     = 1.10 B\n",
      "print_info: general.name     = tinyllama_tinyllama-1.1b-chat-v1.0\n",
      "print_info: vocab type       = SPM\n",
      "print_info: n_vocab          = 32000\n",
      "print_info: n_merges         = 0\n",
      "print_info: BOS token        = 1 '<s>'\n",
      "print_info: EOS token        = 2 '</s>'\n",
      "print_info: UNK token        = 0 '<unk>'\n",
      "print_info: PAD token        = 2 '</s>'\n",
      "print_info: LF token         = 13 '<0x0A>'\n",
      "print_info: EOG token        = 2 '</s>'\n",
      "print_info: max token length = 48\n",
      "load_tensors: loading model tensors, this can take a while... (mmap = true)\n",
      "load_tensors: layer   0 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   1 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   2 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   3 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   4 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   5 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   6 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   7 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   8 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   9 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  10 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  11 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  12 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  13 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  14 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  15 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  16 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  17 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  18 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  19 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  20 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  21 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  22 assigned to device CPU, is_swa = 0\n",
      "load_tensors: tensor 'token_embd.weight' (q4_K) (and 66 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead\n",
      "load_tensors:  CPU_AARCH64 model buffer size =   455.06 MiB\n",
      "load_tensors:   CPU_Mapped model buffer size =   636.18 MiB\n",
      "repack: repack tensor blk.0.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.0.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.0.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.0.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.0.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.1.attn_q.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.1.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.1.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.1.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.1.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.2.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.2.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.2.attn_v.weight with q4_K_8x8\n",
      "repack: repack tensor blk.2.attn_output.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.2.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.2.ffn_down.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.2.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.3.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.3.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.3.attn_v.weight with q4_K_8x8\n",
      "repack: repack tensor blk.3.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.3.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.3.ffn_down.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.3.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.4.attn_q.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.4.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.4.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.4.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.4.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.5.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.5.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.5.attn_v.weight with q4_K_8x8\n",
      "repack: repack tensor blk.5.attn_output.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.5.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.5.ffn_down.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.5.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.6.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.6.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.6.attn_v.weight with q4_K_8x8\n",
      "repack: repack tensor blk.6.attn_output.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.6.ffn_gate.weight with q4_K_8x8\n",
      "repack: repack tensor blk.6.ffn_down.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.6.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.7.attn_q.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.7.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.7.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.7.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.7.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.8.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.8.attn_k.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.8.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.8.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.8.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.9.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.9.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.9.attn_output.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.9.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.9.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.10.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.10.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.10.attn_v.weight with q4_K_8x8\n",
      "repack: repack tensor blk.10.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.10.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.10.ffn_down.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.10.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.11.attn_q.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.11.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.11.attn_v.weight with q4_K_8x8\n",
      "repack: repack tensor blk.11.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.11.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.11.ffn_down.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.11.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.12.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.12.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.12.attn_output.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.12.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.12.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.13.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.13.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.13.attn_v.weight with q4_K_8x8\n",
      "repack: repack tensor blk.13.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.13.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.13.ffn_down.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.13.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.14.attn_q.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.14.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.14.attn_v.weight with q4_K_8x8\n",
      "repack: repack tensor blk.14.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.14.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.14.ffn_down.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.14.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.15.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.15.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.15.attn_output.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.15.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.15.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.16.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.16.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.16.attn_v.weight with q4_K_8x8\n",
      "repack: repack tensor blk.16.attn_output.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.16.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.16.ffn_down.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.16.ffn_up.weight with q4_K_8x8\n",
      "repack: repack tensor blk.17.attn_q.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.17.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.17.attn_v.weight with q4_K_8x8\n",
      "repack: repack tensor blk.17.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.17.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.17.ffn_down.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.17.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.18.attn_q.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.18.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.18.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.18.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.18.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.19.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.19.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.19.attn_v.weight with q4_K_8x8\n",
      "repack: repack tensor blk.19.attn_output.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.19.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.19.ffn_down.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.19.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.20.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.20.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.20.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.20.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.20.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.21.attn_q.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.21.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.21.attn_v.weight with q4_K_8x8\n",
      "repack: repack tensor blk.21.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.21.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.21.ffn_down.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.21.ffn_up.weight with q4_K_8x8\n",
      "..............\n",
      "llama_context: constructing llama_context\n",
      "llama_context: n_seq_max     = 1\n",
      "llama_context: n_ctx         = 512\n",
      "llama_context: n_ctx_per_seq = 512\n",
      "llama_context: n_batch       = 512\n",
      "llama_context: n_ubatch      = 512\n",
      "llama_context: causal_attn   = 1\n",
      "llama_context: flash_attn    = 0\n",
      "llama_context: freq_base     = 10000.0\n",
      "llama_context: freq_scale    = 1\n",
      "llama_context: n_ctx_per_seq (512) < n_ctx_train (2048) -- the full capacity of the model will not be utilized\n",
      "set_abort_callback: call\n",
      "llama_context:        CPU  output buffer size =     0.12 MiB\n",
      "create_memory: n_ctx = 512 (padded)\n",
      "llama_kv_cache_unified: kv_size = 512, type_k = 'f16', type_v = 'f16', n_layer = 22, can_shift = 1, padding = 32\n",
      "llama_kv_cache_unified: layer   0: dev = CPU\n",
      "llama_kv_cache_unified: layer   1: dev = CPU\n",
      "llama_kv_cache_unified: layer   2: dev = CPU\n",
      "llama_kv_cache_unified: layer   3: dev = CPU\n",
      "llama_kv_cache_unified: layer   4: dev = CPU\n",
      "llama_kv_cache_unified: layer   5: dev = CPU\n",
      "llama_kv_cache_unified: layer   6: dev = CPU\n",
      "llama_kv_cache_unified: layer   7: dev = CPU\n",
      "llama_kv_cache_unified: layer   8: dev = CPU\n",
      "llama_kv_cache_unified: layer   9: dev = CPU\n",
      "llama_kv_cache_unified: layer  10: dev = CPU\n",
      "llama_kv_cache_unified: layer  11: dev = CPU\n",
      "llama_kv_cache_unified: layer  12: dev = CPU\n",
      "llama_kv_cache_unified: layer  13: dev = CPU\n",
      "llama_kv_cache_unified: layer  14: dev = CPU\n",
      "llama_kv_cache_unified: layer  15: dev = CPU\n",
      "llama_kv_cache_unified: layer  16: dev = CPU\n",
      "llama_kv_cache_unified: layer  17: dev = CPU\n",
      "llama_kv_cache_unified: layer  18: dev = CPU\n",
      "llama_kv_cache_unified: layer  19: dev = CPU\n",
      "llama_kv_cache_unified: layer  20: dev = CPU\n",
      "llama_kv_cache_unified: layer  21: dev = CPU\n",
      "llama_kv_cache_unified:        CPU KV buffer size =    11.00 MiB\n",
      "llama_kv_cache_unified: KV self size  =   11.00 MiB, K (f16):    5.50 MiB, V (f16):    5.50 MiB\n",
      "llama_context: enumerating backends\n",
      "llama_context: backend_ptrs.size() = 1\n",
      "llama_context: max_nodes = 65536\n",
      "llama_context: worst-case: n_tokens = 512, n_seqs = 1, n_outputs = 0\n",
      "llama_context: reserving graph for n_tokens = 512, n_seqs = 1\n",
      "llama_context: reserving graph for n_tokens = 1, n_seqs = 1\n",
      "llama_context: reserving graph for n_tokens = 512, n_seqs = 1\n",
      "llama_context:        CPU compute buffer size =    66.50 MiB\n",
      "llama_context: graph nodes  = 754\n",
      "llama_context: graph splits = 1\n",
      "CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | LLAMAFILE = 1 | OPENMP = 1 | AARCH64_REPACK = 1 | \n",
      "Model metadata: {'general.name': 'tinyllama_tinyllama-1.1b-chat-v1.0', 'general.architecture': 'llama', 'llama.context_length': '2048', 'llama.rope.dimension_count': '64', 'llama.embedding_length': '2048', 'llama.block_count': '22', 'llama.feed_forward_length': '5632', 'llama.attention.head_count': '32', 'tokenizer.ggml.eos_token_id': '2', 'general.file_type': '15', 'llama.attention.head_count_kv': '4', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.freq_base': '10000.000000', 'tokenizer.ggml.model': 'llama', 'general.quantization_version': '2', 'tokenizer.ggml.bos_token_id': '1', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.padding_token_id': '2', 'tokenizer.chat_template': \"{% for message in messages %}\\n{% if message['role'] == 'user' %}\\n{{ '<|user|>\\n' + message['content'] + eos_token }}\\n{% elif message['role'] == 'system' %}\\n{{ '<|system|>\\n' + message['content'] + eos_token }}\\n{% elif message['role'] == 'assistant' %}\\n{{ '<|assistant|>\\n'  + message['content'] + eos_token }}\\n{% endif %}\\n{% if loop.last and add_generation_prompt %}\\n{{ '<|assistant|>' }}\\n{% endif %}\\n{% endfor %}\"}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Using gguf chat template: {% for message in messages %}\n",
      "{% if message['role'] == 'user' %}\n",
      "{{ '<|user|>\n",
      "' + message['content'] + eos_token }}\n",
      "{% elif message['role'] == 'system' %}\n",
      "{{ '<|system|>\n",
      "' + message['content'] + eos_token }}\n",
      "{% elif message['role'] == 'assistant' %}\n",
      "{{ '<|assistant|>\n",
      "'  + message['content'] + eos_token }}\n",
      "{% endif %}\n",
      "{% if loop.last and add_generation_prompt %}\n",
      "{{ '<|assistant|>' }}\n",
      "{% endif %}\n",
      "{% endfor %}\n",
      "Using chat eos_token: </s>\n",
      "Using chat bos_token: <s>\n",
      "Evaluating bm25: 100%|██████████| 30/30 [00:00<00:00, 394.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 BM25 Evaluation: {'MAP@10': 0.961111111111111, 'MRR@10': 0.961111111111111, 'nDCG@10': 0.9710309917857153}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating dense:   0%|          | 0/30 [00:00<?, ?it/s]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a1a631416af403e866b6e87ac3f6f9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating dense:   3%|▎         | 1/30 [00:00<00:03,  7.44it/s]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94c65286605640ed9360db3d627b981f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb637eefd7ac441c9065e1a128cc8b0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb8046635c53488b9a8551899aa83c6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ffb904ce3c2e43ff9fb2357bdf9b6fb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6eb72b6c7d544fc8e6e532872030846",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating dense:  20%|██        | 6/30 [00:00<00:00, 28.14it/s]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a55e8227350542eabadccacaa3ee8b25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d590ff2713a04609b4a36d1e34393e77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9a234a09abe444cbf26adbe7e2a79c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f8fd6a4b21a4727bf3e587d0bd90ab4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "278d8538dcbc434dab11f8f2d42b01a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating dense:  37%|███▋      | 11/30 [00:00<00:00, 33.11it/s]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bab0bbf53377496a8e69d9a01f9a0437",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8190fa794e34175a15efd8c61fa68f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab87bc3a2f934bd6ad696a5060627930",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf5ab932e3094749827fd52da4c93d38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4484b3ee1a1446fc934e5f7fd33cb398",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating dense:  53%|█████▎    | 16/30 [00:00<00:00, 36.72it/s]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a54200e565a7468ab4bf1acbf1c57082",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9093e786316b4e5d99ab6d26280efd90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "794c49e8db174fa0bbbb2f95bee3b12b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf61b78b0e124d14bbc28015c0222e04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating dense:  67%|██████▋   | 20/30 [00:00<00:00, 36.96it/s]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c7d1b65ff4c4764887c7b38d785c111",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a22b1e1e20fb4ea9bb82c68658880472",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0bcac364c9b0498c9e704e8d0e5d4ffa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "690402b3eb03460597c2b798d9b8bb25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating dense:  80%|████████  | 24/30 [00:00<00:00, 37.44it/s]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c7e6c4aaa7e4fb99bfa7465fbf6609e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d8ca19e2bee4df280b71bfd4e1c3a57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7632f6296c647fda48211ac61e66b92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a064252d26ce47b19a6920f835d5d806",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ec897f90fc345a7be8aac953534d2e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating dense:  97%|█████████▋| 29/30 [00:00<00:00, 40.45it/s]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f4d5925c0c0436b90c3c87ab24b4146",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating dense: 100%|██████████| 30/30 [00:00<00:00, 36.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Dense Evaluation: {'MAP@10': 1.0, 'MRR@10': 1.0, 'nDCG@10': 1.0}\n",
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbb6f453aa1042158150c57bf2163a1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    2965.17 ms\n",
      "llama_perf_context_print: prompt eval time =    2964.50 ms /   390 tokens (    7.60 ms per token,   131.56 tokens per second)\n",
      "llama_perf_context_print:        eval time =      58.30 ms /     2 runs   (   29.15 ms per token,    34.31 tokens per second)\n",
      "llama_perf_context_print:       total time =    3024.60 ms /   392 tokens\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51d36fd9a1334b1a94f161afcb4bf273",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 9 prefix-match hit, remaining 319 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2965.17 ms\n",
      "llama_perf_context_print: prompt eval time =    2395.99 ms /   319 tokens (    7.51 ms per token,   133.14 tokens per second)\n",
      "llama_perf_context_print:        eval time =     880.08 ms /    39 runs   (   22.57 ms per token,    44.31 tokens per second)\n",
      "llama_perf_context_print:       total time =    3289.83 ms /   358 tokens\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8c6c35c46ea4bf194d6dd78a41abc68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 9 prefix-match hit, remaining 461 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2965.17 ms\n",
      "llama_perf_context_print: prompt eval time =    3327.74 ms /   461 tokens (    7.22 ms per token,   138.53 tokens per second)\n",
      "llama_perf_context_print:        eval time =     964.50 ms /    41 runs   (   23.52 ms per token,    42.51 tokens per second)\n",
      "llama_perf_context_print:       total time =    4308.23 ms /   502 tokens\n"
     ]
    }
   ],
   "source": [
    "# 📚 Imports\n",
    "import json\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from rank_bm25 import BM25Okapi\n",
    "import nltk\n",
    "import numpy as np\n",
    "import faiss\n",
    "import gradio as gr\n",
    "from collections import defaultdict\n",
    "import os\n",
    "import re\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from llama_cpp import Llama\n",
    "from tqdm import tqdm\n",
    "import pytrec_eval  # Added for proper evaluation\n",
    "\n",
    "# ✅ Download punkt tokenizer if missing\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "except LookupError:\n",
    "    nltk.download(\"punkt\")\n",
    "\n",
    "# 📂 Dataset paths\n",
    "DATA_DIR = \"stacklite_dataset\"\n",
    "DATA_FILE = os.path.join(DATA_DIR, \"top_datascience_questions.json\")\n",
    "QUERY_FILE = os.path.join(DATA_DIR, \"queries.json\")\n",
    "QRELS_FILE = os.path.join(DATA_DIR, \"qrels.json\")\n",
    "\n",
    "# ✅ Ensure files exist\n",
    "for file in [DATA_FILE, QUERY_FILE, QRELS_FILE]:\n",
    "    if not os.path.exists(file):\n",
    "        raise FileNotFoundError(f\"Missing file: {file}. Please place it in {DATA_DIR}\")\n",
    "\n",
    "# 📑 Load dataset\n",
    "with open(DATA_FILE, 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# 🧼 Clean + preprocess\n",
    "def clean_text(html):\n",
    "    return BeautifulSoup(html, \"html.parser\").get_text(separator=\" \")\n",
    "\n",
    "df['text'] = df.apply(lambda row: row['title'] + \" \" + clean_text(row['body']), axis=1)\n",
    "\n",
    "# 🔍 BM25\n",
    "def safe_tokenize(text):\n",
    "    try:\n",
    "        return nltk.word_tokenize(text.lower())\n",
    "    except LookupError:\n",
    "        return re.findall(r'\\b\\w+\\b', text.lower())\n",
    "\n",
    "tokenized_corpus = [safe_tokenize(doc) for doc in df['text']]\n",
    "bm25 = BM25Okapi(tokenized_corpus)\n",
    "\n",
    "# 🤖 Dense Embeddings (MiniLM)\n",
    "embedding_model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "def get_embeddings(texts, batch_size=32):\n",
    "    return embedding_model.encode(texts, batch_size=batch_size, show_progress_bar=True)\n",
    "\n",
    "# 📐 FAISS index\n",
    "dense_embeddings = get_embeddings(df['text'].tolist())\n",
    "dim = dense_embeddings.shape[1]\n",
    "index = faiss.IndexFlatL2(dim)\n",
    "index.add(dense_embeddings.astype(np.float32))\n",
    "\n",
    "# 🔁 RRF Fusion\n",
    "def rrf(bm25_ids, dense_ids, k=60):\n",
    "    scores = defaultdict(float)\n",
    "    for rank, idx in enumerate(bm25_ids):\n",
    "        scores[idx] += 1 / (k + rank + 1)\n",
    "    for rank, idx in enumerate(dense_ids):\n",
    "        scores[idx] += 1 / (k + rank + 1)\n",
    "    return sorted(scores.keys(), key=scores.get, reverse=True)\n",
    "\n",
    "# 🧠 Local Chat LLM (TinyLlama or similar, GGUF)\n",
    "llm = Llama(model_path=\"./models/tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf\")  # ✅ Update path as needed\n",
    "\n",
    "def generate_answer(prompt):\n",
    "    output = llm(prompt, max_tokens=300, stop=[\"\\n\\n\", \"User:\"], echo=False)\n",
    "    return output[\"choices\"][0][\"text\"].strip()\n",
    "\n",
    "# 🧪 Load Queries + Qrels\n",
    "with open(QUERY_FILE, \"r\", encoding='utf-8') as f:\n",
    "    queries = json.load(f)\n",
    "with open(QRELS_FILE, \"r\", encoding='utf-8') as f:\n",
    "    qrels = json.load(f)\n",
    "\n",
    "# 📊 Evaluation using pytrec_eval\n",
    "def evaluate_search(method=\"bm25\", top_k=10):\n",
    "    # Prepare qrels in pytrec_eval format\n",
    "    qrels_dict = {str(qid): {str(doc): 1 for doc in docs} for qid, docs in qrels.items()}\n",
    "    \n",
    "    # Build run dictionary\n",
    "    run_dict = {}\n",
    "    for q in tqdm(queries, desc=f\"Evaluating {method}\"):\n",
    "        qid = str(q['id'])\n",
    "        text = q['question']\n",
    "        \n",
    "        if method == \"bm25\":\n",
    "            tok_q = safe_tokenize(text)\n",
    "            scores = bm25.get_scores(tok_q)\n",
    "            top_idxs = np.argsort(scores)[::-1][:top_k]\n",
    "            run_dict[qid] = {str(df.iloc[i]['question_id']): float(scores[i]) for i in top_idxs}\n",
    "        else:\n",
    "            q_embedding = get_embeddings([text])\n",
    "            distances, top_idxs = index.search(q_embedding.astype(np.float32), top_k)\n",
    "            # Use negative distance as score (since smaller distance = better)\n",
    "            run_dict[qid] = {str(df.iloc[i]['question_id']): float(-distances[0][j]) \n",
    "                            for j, i in enumerate(top_idxs[0])}\n",
    "    \n",
    "    # Evaluate\n",
    "    evaluator = pytrec_eval.RelevanceEvaluator(qrels_dict, {'map', 'recip_rank', 'ndcg'})\n",
    "    metrics = evaluator.evaluate(run_dict)\n",
    "    \n",
    "    # Aggregate metrics\n",
    "    return {\n",
    "        \"MAP@10\": np.mean([v['map'] for v in metrics.values()]),\n",
    "        \"MRR@10\": np.mean([v['recip_rank'] for v in metrics.values()]),\n",
    "        \"nDCG@10\": np.mean([v['ndcg'] for v in metrics.values()])\n",
    "    }\n",
    "\n",
    "# ✅ Run evaluation\n",
    "print(\"🔍 BM25 Evaluation:\", evaluate_search(\"bm25\"))\n",
    "print(\"🔍 Dense Evaluation:\", evaluate_search(\"dense\"))\n",
    "\n",
    "# 💬 Hybrid QA\n",
    "def hybrid_answer(query):\n",
    "    tok_q = safe_tokenize(query)\n",
    "    bm_scores = bm25.get_scores(tok_q)\n",
    "    bm_top = np.argsort(bm_scores)[::-1][:30]\n",
    "    \n",
    "    q_embedding = get_embeddings([query])\n",
    "    _, dense_top = index.search(q_embedding.astype(np.float32), 30)\n",
    "    dense_top = dense_top[0]\n",
    "    \n",
    "    hybrid_ids = rrf(bm_top, dense_top)\n",
    "    MAX_DOC_TOKENS = 100  # Tune based on your tests\n",
    "\n",
    "    def truncate_text(text, max_words=100):\n",
    "        return ' '.join(text.split()[:max_words])\n",
    "\n",
    "    top_k_docs = [truncate_text(df.iloc[i]['text'], max_words=MAX_DOC_TOKENS) for i in hybrid_ids[:3] if i < len(df)]\n",
    "    context = \"\\n\\n\".join([f\"[Doc {i+1}]: {doc}\" for i, doc in enumerate(top_k_docs)])\n",
    "\n",
    "    prompt = f\"\"\"Context:\n",
    "{context}\n",
    "\n",
    "Question: {query}\n",
    "\n",
    "Answer:\"\"\"\n",
    "    answer = generate_answer(prompt)\n",
    "\n",
    "    citations = []\n",
    "    for i, idx in enumerate(hybrid_ids[:5]):\n",
    "        if idx < len(df):\n",
    "            title = df.iloc[idx]['title']\n",
    "            question_id = df.iloc[idx]['question_id']\n",
    "            citations.append(f\"{i+1}. {title} (ID: {question_id})\")\n",
    "    \n",
    "    return answer, \"\\n\".join(citations)\n",
    "\n",
    "# 🚀 Gradio Interface\n",
    "iface = gr.Interface(\n",
    "    fn=hybrid_answer,\n",
    "    inputs=gr.Textbox(label=\"Technical Question\", placeholder=\"Ask a data science question...\"),\n",
    "    outputs=[\n",
    "        gr.Textbox(label=\"Generated Answer\"),\n",
    "        gr.Textbox(label=\"Source Citations\")\n",
    "    ],\n",
    "    title=\"📚 Hybrid QA Assistant (StackLite)\",\n",
    "    description=\"\"\"🔍 Ask technical questions! This assistant uses:\n",
    "- BM25 for keyword search\n",
    "- MiniLM embeddings for semantic search\n",
    "- Hybrid ranking (RRF fusion)\n",
    "- Local LLM (TinyLlama via llama-cpp) for answer generation\"\"\",\n",
    "    examples=[\n",
    "        \"How to handle missing values in a dataset?\",\n",
    "    \"What's the difference between random forest and gradient boosting?\",\n",
    "    \"How to implement a neural network in PyTorch?\",\n",
    "    \"When should I use L1 vs L2 regularization?\",\n",
    "    \"How does PCA reduce dimensionality?\",\n",
    "    \"Explain cross-validation and why it's important.\",\n",
    "    \"How to optimize hyperparameters in scikit-learn?\",\n",
    "    \"What is the bias-variance tradeoff?\",\n",
    "    \"How to deploy a model with Flask?\",\n",
    "    \"What are attention mechanisms in transformers?\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    iface.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "llamma produced better output than gpt 2 in my case and scored better "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
